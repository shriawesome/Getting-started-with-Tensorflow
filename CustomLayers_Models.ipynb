{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialising tensors of 0\n",
    "zero_init=tf.zeros_initializer()\n",
    "tensor_1=zero_init(shape=(2,3))\n",
    "tensor_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float32, numpy=\n",
       "array([[0., 0., 0.],\n",
       "       [0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same as above\n",
    "tf_zeros=tf.zeros((2,3))\n",
    "tf_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 5), dtype=float32, numpy=\n",
       "array([[-0.00902745, -0.03056854, -0.07194978,  0.00245987,  0.02198003],\n",
       "       [ 0.05937332, -0.00403734, -0.02727481,  0.06275862,  0.0086606 ],\n",
       "       [ 0.02383982,  0.02031765, -0.0247496 , -0.03660399,  0.09084921],\n",
       "       [ 0.10113718,  0.00522245,  0.03222394,  0.00604396, -0.03636928]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Normal initializer\n",
    "random_init=tf.random_normal_initializer()\n",
    "tensor_2=random_init((4,5))\n",
    "tensor_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 5), dtype=float32, numpy=\n",
       "array([[-0.5550387 , -1.3227407 ,  0.10742939,  0.5754714 ,  2.6015885 ],\n",
       "       [ 0.04821255,  1.9442492 ,  0.21065842,  0.42030582,  0.19040067],\n",
       "       [-1.7112308 , -0.62808806, -0.53258026, -0.2225895 , -0.6429794 ],\n",
       "       [ 2.9819388 , -0.7575359 , -2.2434952 , -1.0513608 ,  0.43877855]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same as above\n",
    "tf_rand=tf.random.normal((4,5))\n",
    "tf_rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing a Simple Linear NN forward pass\n",
    "Layer encapsulates state and the transformation from inputs to outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinear(layers.Layer):\n",
    "    \"\"\"\n",
    "    SimpleLinear\n",
    "    ------------------------------------\n",
    "    Parameters :\n",
    "    \n",
    "    input_dims : the total number of input features\n",
    "    units : total number of neurons in the layer\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self,input_dims=16,units=8,**kwargs):\n",
    "        super(SimpleLinear,self).__init__(**kwargs)\n",
    "        \n",
    "        w_init=tf.random_normal_initializer()\n",
    "        self.w=tf.Variable(initial_value=w_init(shape=(input_dims,units),\n",
    "                                               dtype='float32'),\n",
    "                          trainable=True)\n",
    "        \n",
    "        b_init=tf.zeros_initializer()\n",
    "        self.b=tf.Variable(initial_value=b_init((units),dtype='float32'),\n",
    "                          trainable=True)\n",
    "        \n",
    "    def call(self,input_tensors):\n",
    "        return tf.matmul(input_tensors,self.w) +self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
       "array([[1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.],\n",
       "       [1., 1., 1.]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining a random input tensor\n",
    "x=tf.ones((4,3))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-0.03397065  0.2308265  -0.0325742   0.02061169 -0.05616296]\n",
      " [-0.03397065  0.2308265  -0.0325742   0.02061169 -0.05616296]\n",
      " [-0.03397065  0.2308265  -0.0325742   0.02061169 -0.05616296]\n",
      " [-0.03397065  0.2308265  -0.0325742   0.02061169 -0.05616296]], shape=(4, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "layer1=SimpleLinear(input_dims=3,units=5)\n",
    "\n",
    "y=layer1(x)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
       " array([[ 0.0022761 ,  0.08788048,  0.03339681,  0.01799115,  0.02110327],\n",
       "        [-0.06592079,  0.10671709,  0.00838614, -0.02669026, -0.02795597],\n",
       "        [ 0.02967404,  0.03622892, -0.07435714,  0.02931081, -0.04931026]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(5,) dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the weights\n",
    "layer1.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(3, 5) dtype=float32, numpy=\n",
       " array([[ 0.0022761 ,  0.08788048,  0.03339681,  0.01799115,  0.02110327],\n",
       "        [-0.06592079,  0.10671709,  0.00838614, -0.02669026, -0.02795597],\n",
       "        [ 0.02967404,  0.03622892, -0.07435714,  0.02931081, -0.04931026]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(5,) dtype=float32, numpy=array([0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the trainable variables that prints both the weights and the \n",
    "# biases associated with the layers.\n",
    "layer1.trainable_variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the nontrainable variables and the losses associated with \n",
    "# each layer\n",
    "layer1.non_trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2\n",
    "Most of the times if the input features are not known to the user then we can initialise the model via this method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinear(layers.Layer):\n",
    "    \n",
    "    def __init__(self,units=8,**kwargs):\n",
    "        super().__init__(self,**kwargs)\n",
    "        self.units=units\n",
    "        \n",
    "    # During the forward pass, the layer will automatically call the build()\n",
    "    # method to determine the shape of the inputs and initialize the weights\n",
    "    def build(self,input_shape):\n",
    "        self.w=self.add_weight(shape=(input_shape[-1],self.units),\n",
    "                              initializer='random_normal',\n",
    "                              trainable=True)\n",
    "        \n",
    "        self.b=self.add_weight(shape=(self.units),\n",
    "                              initializer='zeros',\n",
    "                              trainable=True)\n",
    "        \n",
    "    def call(self,input_tensor):\n",
    "        return tf.matmul(input_tensor,self.w)+self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 5), dtype=float32, numpy=\n",
       "array([[ 0.02411762, -0.03994356, -0.03458344,  0.03353811,  0.01433596],\n",
       "       [-0.00023986,  0.00025884, -0.04324535,  0.03928411,  0.00372976],\n",
       "       [ 0.02734495, -0.01007019, -0.02478502,  0.00704491, -0.01495076],\n",
       "       [ 0.0127981 ,  0.02194271, -0.0378764 , -0.02696394, -0.03290088]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing input\n",
    "random_uniform=tf.random_uniform_initializer()\n",
    "x=random_uniform((4,5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
       "array([[-1.4922817e-03,  4.5631924e-03, -8.5419957e-03],\n",
       "       [ 9.4598045e-06,  1.8515445e-03, -6.1756135e-03],\n",
       "       [ 7.3073042e-04,  9.8680716e-04, -2.1433735e-03],\n",
       "       [ 2.2717128e-03, -3.5976470e-03,  3.3584146e-03]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initializing a class without the shape of the input variable\n",
    "layers2=SimpleLinear(units=3)\n",
    "y=layers2(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'simple_linear_1/Variable:0' shape=(5, 3) dtype=float32, numpy=\n",
       " array([[-0.06778654,  0.00639484, -0.01494635],\n",
       "        [-0.04271008, -0.06950284,  0.0590882 ],\n",
       "        [ 0.00300944,  0.01264456,  0.05634607],\n",
       "        [ 0.01682263,  0.06149903, -0.08999886],\n",
       "        [-0.14115177,  0.00052345, -0.05959132]], dtype=float32)>,\n",
       " <tf.Variable 'simple_linear_1/Variable:0' shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers2.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising a Simple Linear NN with more than 1 layer forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear NN with \n",
    "class SimpleLinearBlock(layers.Layer):\n",
    "    \n",
    "    def __init__(self,block1_units=2,block2_units=4,block3_units=8,**kwargs):\n",
    "        super().__init__(self,**kwargs)\n",
    "        self.linear_1=SimpleLinear(block1_units)\n",
    "        self.linear_2=SimpleLinear(block2_units)\n",
    "        self.linear_3=SimpleLinear(block3_units)\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        x=self.linear_1(inputs)\n",
    "        x=tf.nn.relu(x)\n",
    "        \n",
    "        x=self.linear_2(x)\n",
    "        x=tf.nn.relu(x)\n",
    "        \n",
    "        return self.linear_3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 8), dtype=float32, numpy=\n",
       "array([[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "       [ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00,\n",
       "         0.0000000e+00,  0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
       "       [ 1.4656979e-06,  7.3210441e-07,  1.3714816e-06, -3.1525481e-06,\n",
       "        -1.6354838e-06,  2.5373045e-06,  1.7343017e-06, -2.6770113e-06],\n",
       "       [ 4.6346877e-06,  2.2813190e-06,  4.3640625e-06, -9.8525725e-06,\n",
       "        -5.1009711e-06,  7.8948960e-06,  5.4317688e-06, -8.3710447e-06]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiating the object of the class\n",
    "simple_linear_block=SimpleLinearBlock()\n",
    "y=simple_linear_block(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'simple_linear_block/simple_linear_2/Variable:0' shape=(5, 2) dtype=float32, numpy=\n",
       " array([[ 0.00919736,  0.05933924],\n",
       "        [ 0.01814552,  0.06530889],\n",
       "        [ 0.03996212, -0.01253904],\n",
       "        [-0.04761524, -0.02249529],\n",
       "        [-0.00035445, -0.02187215]], dtype=float32)>,\n",
       " <tf.Variable 'simple_linear_block/simple_linear_2/Variable:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'simple_linear_block/simple_linear_3/Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
       " array([[ 2.0009888e-02, -8.3881030e-03,  3.1028295e-02,  7.6694683e-05],\n",
       "        [ 1.2971694e-02, -1.2309920e-01,  1.6123127e-02, -3.7103418e-02]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'simple_linear_block/simple_linear_3/Variable:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'simple_linear_block/simple_linear_4/Variable:0' shape=(4, 8) dtype=float32, numpy=\n",
       " array([[-0.00014113,  0.02275375, -0.01863571, -0.07840307, -0.04769793,\n",
       "          0.08674123,  0.03526624, -0.06342129],\n",
       "        [-0.01891675, -0.07296854, -0.04952801,  0.01678478, -0.00727292,\n",
       "          0.01117639, -0.04482498, -0.04244238],\n",
       "        [ 0.06305671,  0.01313332,  0.07389029, -0.07230517, -0.03185961,\n",
       "          0.03917566,  0.04610505, -0.06393706],\n",
       "        [-0.01170382,  0.02164177,  0.08862864,  0.04088091,  0.03193776,\n",
       "          0.07430219,  0.05609378,  0.12933831]], dtype=float32)>,\n",
       " <tf.Variable 'simple_linear_block/simple_linear_4/Variable:0' shape=(8,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# includes weights of each \n",
    "simple_linear_block.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing loss associated with the forward pass\n",
    "class RegularizationLoss(layers.Layer):\n",
    "    def __init__(self,rate=1e-3,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.rate=rate\n",
    "        \n",
    "    # layers recursively collects errors from sub layers during forward loss   \n",
    "    def call(self,inputs):\n",
    "        self.add_loss(self.rate*tf.reduce_sum(inputs))\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 5), dtype=float32, numpy=\n",
       "array([[ 0.02411762, -0.03994356, -0.03458344,  0.03353811,  0.01433596],\n",
       "       [-0.00023986,  0.00025884, -0.04324535,  0.03928411,  0.00372976],\n",
       "       [ 0.02734495, -0.01007019, -0.02478502,  0.00704491, -0.01495076],\n",
       "       [ 0.0127981 ,  0.02194271, -0.0378764 , -0.02696394, -0.03290088]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_loss_layer = RegularizationLoss()\n",
    "\n",
    "y=reg_loss_layer(x)\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=-8.116433e-05>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# losses assciated with each layer is present in the losses\n",
    "reg_loss_layer.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composing a Regularized layer with the Outer layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinearRegularized(layers.Layer):\n",
    "    def __init__(self,units=8,**kwargs):\n",
    "        super().__init__(self,**kwargs)\n",
    "        \n",
    "        self.units=units\n",
    "        self.reg=RegularizationLoss(1e-2)\n",
    "        \n",
    "    def build(self,input_shape):\n",
    "        self.w=self.add_weight(shape=(input_shape[-1],self.units),\n",
    "                              initializer='random_normal',\n",
    "                              trainable=True)\n",
    "        \n",
    "        self.b=self.add_weight(shape=(self.units),\n",
    "                              initializer='ones',\n",
    "                              trainable=True)\n",
    "        \n",
    "    def call(self,input_tensors):\n",
    "        output=tf.matmul(input_tensors,self.w) + self.b\n",
    "        \n",
    "        return self.reg(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
       "array([[0.99896306, 1.0027885 , 0.9961299 ],\n",
       "       [1.0018696 , 1.0018736 , 0.99976254],\n",
       "       [1.0014498 , 0.9995614 , 0.99870527],\n",
       "       [1.0045989 , 0.99681866, 1.0020354 ]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y is the same as the input x but there is a loss associated with it \n",
    "# which was previously 0\n",
    "layers3=SimpleLinearRegularized(units=3)\n",
    "y=layers3(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=0.120045565>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers3.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Dense Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseRegularized(layers.Layer):\n",
    "    def __init__(self,units=8,**kwargs):\n",
    "        super().__init__(self,**kwargs)\n",
    "        self.dense=layers.Dense(units,kernel_regularizer=tf.keras.regularizers.l2(1e-3))\n",
    "        \n",
    "        self.reg=RegularizationLoss(1e-2)\n",
    "        \n",
    "    def call(self,input_tensor):\n",
    "        return self.reg(self.dense(input_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
       "array([[ 0.01570651,  0.00987195, -0.00507249],\n",
       "       [ 0.00013045,  0.0041152 ,  0.00858237],\n",
       "       [-0.00616979,  0.0227704 ,  0.01035901],\n",
       "       [-0.01472204,  0.04903283,  0.00925154]], dtype=float32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer5=DenseRegularized(units=3)\n",
    "y=layer5(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(), dtype=float32, numpy=0.0031004439>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=0.0010385594>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1st is the loss associated with the dense keras regularizer\n",
    "# and 2nd is the loss associated with the custom loss\n",
    "layer5.losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serializing the Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLinear(layers.Layer):\n",
    "    \n",
    "    def __init__(self,units=8,**kwargs):\n",
    "        super(SimpleLinear,self).__init__(**kwargs)\n",
    "        self.units=units\n",
    "        \n",
    "    # Helps with the weight initialization\n",
    "    def build(self,input_shape):\n",
    "        self.w=self.add_weight(shape=(input_shape[-1],self.units),\n",
    "                              initializer='random_normal',\n",
    "                              trainable=True)\n",
    "        \n",
    "        self.b=self.add_weight(shape=(self.units),\n",
    "                              initializer = 'ones',\n",
    "                              trainable=True)\n",
    "        \n",
    "        \n",
    "    # Helps with the forward pass    \n",
    "    def call(self,input_tensor):\n",
    "        return tf.matmul(input_tensor,self.w)+self.b\n",
    "    \n",
    "    # Useful for Serializing the present Layer\n",
    "    def get_config(self):\n",
    "        config=super().get_config()\n",
    "        config.update({'units':self.units})\n",
    "        \n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.000366   1.0042981  0.9952282 ]\n",
      " [1.0019532  1.0028276  0.99594784]\n",
      " [1.0005943  1.0041045  0.9989029 ]\n",
      " [1.0018231  1.0036355  1.000455  ]], shape=(4, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "layer6=SimpleLinear(units=3)\n",
    "y=layer6(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'simple_linear_5', 'trainable': True, 'dtype': 'float32', 'units': 3}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calling the get_config, and in our case we only specified the units\n",
    "config=layer6.get_config()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.0062287  1.0018716  1.0103018 ]\n",
      " [1.0029446  0.99729747 1.0096915 ]\n",
      " [1.0016078  1.0011208  1.003331  ]\n",
      " [0.99820125 1.0004576  1.0004061 ]], shape=(4, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Build a new layer with the same configuration\n",
    "new_layer6=SimpleLinear.from_config(config)\n",
    "print(new_layer6(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDropout(layers.Layer):\n",
    "    \n",
    "    def __init__(self,rate,**kwargs):\n",
    "        super(CustomDropout,self).__init__(**kwargs)\n",
    "        self.rate=rate\n",
    "        \n",
    "    # needs additional parameter 'training' as the Dropout layer behaves\n",
    "    # differently during training and testing.\n",
    "    def call(self,inputs,training=None):\n",
    "        if training:\n",
    "            return tf.nn.dropout(inputs,rate=self.rate)\n",
    "        \n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.02411762 -0.03994356 -0.03458344  0.03353811  0.01433596]\n",
      " [-0.00023986  0.00025884 -0.04324535  0.03928411  0.00372976]\n",
      " [ 0.02734495 -0.01007019 -0.02478502  0.00704491 -0.01495076]\n",
      " [ 0.0127981   0.02194271 -0.0378764  -0.02696394 -0.03290088]], shape=(4, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "layer7=CustomDropout(rate=0.4)\n",
    "\n",
    "print(layer7(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 0.04019604 -0.06657261 -0.05763906  0.05589684  0.        ]\n",
      " [-0.          0.0004314  -0.07207558  0.06547352  0.        ]\n",
      " [ 0.         -0.01678365 -0.04130836  0.         -0.02491794]\n",
      " [ 0.          0.03657118 -0.06312734 -0.         -0.        ]], shape=(4, 5), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Nothing happens above as we have training value as None/False\n",
    "# Adding a dropout layer with training=True, by doing this we'll see\n",
    "# that 40% of the data will now have 0 in place.\n",
    "layer7_dropout=CustomDropout(rate=0.4)\n",
    "print(layer7_dropout(x,training=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRegressionModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,input_shape,layer_units=[8,4],\\\n",
    "                activation='relu',initializer='random_normal'):\n",
    "        super(CustomRegressionModel,self).__init__()\n",
    "        assert len(layer_units)>0\n",
    "        # input layer\n",
    "        self.input_layer=layers.Dense(layer_units[0],activation='relu',\n",
    "                                     kernel_initializer=initializer,\n",
    "                                     input_shape=[input_shape])\n",
    "        \n",
    "        # hidden layers\n",
    "        self.hidden_layers=[]\n",
    "        \n",
    "        for i in range(1,len(layer_units)):\n",
    "            self.hidden_layers.append(layers.Dense(layer_units[i],\n",
    "                                                  activation=activation,\n",
    "                                                  kernel_initializer=initializer))\n",
    "            \n",
    "        # output layer with o/p 1 as it is a Regression problem\n",
    "        self.output_layer=layers.Dense(1)\n",
    "        \n",
    "        \n",
    "    # Forward pass via the call method\n",
    "    def call(self,input_tensor):\n",
    "        x=self.input_layer(input_tensor)\n",
    "        \n",
    "        for layer in self.hidden_layers:\n",
    "            x=layer(x)\n",
    "            \n",
    "        result=self.output_layer(x)\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 8), dtype=float32, numpy=\n",
       "array([[-0.01629189, -0.01485084,  0.01104771,  0.01949776, -0.03587743,\n",
       "        -0.01508524,  0.02445206, -0.04601003],\n",
       "       [ 0.00577383, -0.0115394 ,  0.03660784,  0.02304305,  0.02786363,\n",
       "         0.01227624,  0.00322606, -0.00215416],\n",
       "       [-0.04443649, -0.03986238, -0.03464027, -0.046432  , -0.00904036,\n",
       "         0.02792771, -0.04784675, -0.0184095 ],\n",
       "       [-0.01544676,  0.02086978,  0.03126012, -0.02356156, -0.01701038,\n",
       "        -0.02829906,  0.04033628,  0.00026138]], dtype=float32)>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a input\n",
    "rand_uniform=tf.random_uniform_initializer()\n",
    "x=rand_uniform(shape=(4,8))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.core.Dense at 0x63e8c0898>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x63e8c0da0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x63e885128>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x63e891e48>]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating an object for our Regression Class\n",
    "# 3 layers that we have mentioned and the last one is the output layer\n",
    "custom_reg_model=CustomRegressionModel(x.shape[-1],[8,16,32],'sigmoid')\n",
    "custom_reg_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1), dtype=float32, numpy=\n",
       "array([[-0.22642323],\n",
       "       [-0.22642234],\n",
       "       [-0.22641975],\n",
       "       [-0.22642112]], dtype=float32)>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass for the model, this is needed before we can print the summary \n",
    "# of the model. Output is the prediction made by the model.\n",
    "custom_reg_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"custom_regression_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              multiple                  72        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  144       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  544       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  33        \n",
      "=================================================================\n",
      "Total params: 793\n",
      "Trainable params: 793\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "custom_reg_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goto the RegressionwithCustomLayer.ipynb for the implementation of Custom layer models on Dataset for Regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
